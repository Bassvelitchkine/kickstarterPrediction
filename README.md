# Kickstarter prediction

## Introduction

Kickstarter est une plateforme am√©ricaine de crowd lending. Les porteurs de projet en tout genre lancent leurs campagnes sur kickstarter. Une campagne a une dur√©e limit√©e durant laquelle les investisseurs (potentiellement le commun des mortels) peut d√©cider d'investir de l'argent dans le projet.

Il existe des projets dans des domaines tout √† fait vari√© et √©videmment les succ√®s des campagnes sont variables. On dit qu'une campagne sur kickstarter est r√©ussie lorsque la somme demand√©e par les porteurs de projets a √©t√© apport√©e par les investisseurs.

Ainsi, il peut √™tre int√©ressant pour les porteurs de projet d'avoir une connaissance assez fines des crit√®res de r√©ussite d'une campagne sur kickstarter (travail de visualisation des donn√©es et de data analyst). Mais il est d'autant plus int√©ressant pour les investisseurs de savoir si les projets dans lesquels ils investissent verront le jour ou non. Personne n'a envie de jeter d'argent par les fen√™tres üí∏

_In fine_, nous avons tent√© de pr√©dire la r√©ussite ou l'√©chec d'une campagne kickstarter (probl√®me de classification binaire).

## Avant-propos

Avant de foncer t√™te baiss√©e dans le machine learning pur, nous avons pris le temps de visualiser nos donn√©es, de regarder sur internet ce qui avait d√©j√† √©t√© fait (nous sommes partis d'un notebook trouv√© en ligne et avons tent√© de prolonger la r√©flexion).

Nous avons travaill√© plusieurs sur ce projet, sans nous servir de git. La collaboration a √©t√© maladroite, vous pourrez donc observer de multiples notebooks √† ouvrir avec Jupyter.

## Pipeline

### Data visualization

Nous avons commenc√© par visualiser les donn√©es. L'id√©e √©tait de faire apparaitre des "motifs" lors de la visualisation, des corr√©lations √©videntes, en vain. Je me suis charg√© de cette partie.

### Preprocessing

Il s'agissait de ne garder que les features les plus int√©ressantes et √©ventuellement de r√©duire la dimension pour parvenir √† une meilleure s√©parabilit√©.

### Classification simple

Nous avons tous test√© pl√©thore de mod√®les de classification : na√Øve bay√©sienne, random forest, adaboost, r√©seaux de neurones, etc. Le but √©tait d'identifier les mod√®les les plus performants, les uns ind√©pendemment des autres.

### Stacking

L'id√©e la plus avanc√©e que nous ayons eu a √©t√© de mettre en place des algorithmes de stacking. Il s'agit d'entrainer plusieurs mod√®les sur nos donn√©es, de r√©cup√©rer les plus performants, et de donner les sorties de ces mod√®les en entr√©e d'un nouvel algorithme de classification. L'id√©e est d'avoir des classifieurs plus ou moins faibles et de les combiner pour en faire un classifieur bien plus performant. Notre crit√®re d'√©valuation a √©t√© le score F1.

Je me suis charg√© de cette partie aussi.

## D√©marrage

Comme tout le code est r√©parti dans des notebooks ind√©pendants les uns des autres, je vous conseille de les ouvrir al√©atoirement en vous fiant aux noms qui leur ont √©t√© donn√©s, apr√®s avoir clon√© le d√©p√¥t localement.

Je n'ai pas upload√© les datasets que nous avions trouv√© en ligne car ils sont excessivement lourds et inutiles pour visualiser le travail que nous avons effectu√©.

## Conclusion

Nous avons finalement atteint un score F1 de 76% avec le stacking. C'est un r√©sultat d√©cevant car √† peine plus √©lev√© que celui des classifieurs seuls sans stacking. Nous avons travaill√© en temps limit√© et aurions pu aller plus loin avec plus de temps.

## Authors

- **Bastien Velitchkine** - _Student @ CentraleSup√©lec_ - [Bassvelitchkine](https://github.com/Bassvelitchkine)
- **Merlin Egalit√©** - _Student @ CentraleSup√©lec_ - [MerlinEgalite](https://github.com/MerlinEgalite)
- **Ma√Øwenn Danno** - _Student @ CentraleSup√©lec_
- **Micka√´l De La Roque** - _Student @ CentraleSup√©lec_
